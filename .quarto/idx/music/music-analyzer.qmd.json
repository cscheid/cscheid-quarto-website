{"title":"Notes on musical sound analysis in JS","markdown":{"yaml":{"title":"Notes on musical sound analysis in JS","date":"2022-06-11"},"headingText":"Enemies","containsRefs":false,"markdown":"\n\nI want to build a frequency analyzer for musical sounds that runs on a web browser,\nso that my synthesized sounds have an analyzer next to them.\n\nI'm learning all that stuff as I go along, so: actual audio engineers, you probably will run away screaming here.\nOr don't, and tell me where I'm wrong. That helps too.\n\n\n### Enemy number 1: frequency resolution\n\nI started out thinking that I could use the (very powerful!) [built-in AudioContext analyzer](https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/createAnalyser) in the Web audio API. \nHowever, I realized that it does not provide prefiltering or subsampling of the data.\nThis means that short-window FFTs can't distinguish small differences in low-frequency\nsignals. This seems to make me need large window sizes.\n\nA bass guitar, for example, is tuned to produce notes (roughly) from E1 to G4, which\nhave the lowest overtones ranging 40 to 400Hz. If we want to see 3 octaves of overtones at the highest end, that's \na range from 40 to 3200Hz.\n\nLet's consider $r$ samples/s as the signal rate. ($r = 48K$ typically).\nFor an FFT of size $w$, this gives the lowest non-dc resolvable frequency at $r/2$ samples, or $2r/w$ frequency.\n\n$$r = 48K \\\\ w = 256 \\\\ 2r/w = 48K/256 = 187.5\\textrm{Hz}$$\n\nSolving backwards to get a FFT window that can resolve 40Hz tones, we get $96k/w = 40, w = 2400$. So we need at least a 4096-wide FFT.\nBut that's only the start. There are 12 pitches between 40 and 80Hz, and we would like to be able to distinguish those.\n\nAt the low frequency, the resolution (difference between resolvable frequencies) of the plain FFT is $3r/w - 2r/w = r/w$.\nIn our case, $r/w = 11.71Hz$, so we can see how 4096-wide FFTs can cause trouble. An 11Hz difference in low frequencies is quite noticeable. \nE1 is $41.2\\textrm{Hz}$, and G#1 is $51.9\\textrm{Hz}$. We really would like to be able to distinguish\nEs from F#s, and 4096 samples aren't enough.\n\nIf we want to distinguish between E1 and F1 in a frequency analyzer, we need\n$(43.65 - 41.2)\\textrm{Hz} = 2.45\\textrm{Hz}$ resolution, which works out to be $48000/w = 2.45, w = 19.5k$ samples. That means we\nneed $w = 32768$ to make this work in the context of powers-of-2 FFT windows.\n\n### Enemy number 2: latency\n\nAt $r = 48000$ and $w = 32768$, I really hope that the built-in analyzer uses moving windows, or otherwise we're stuck with high latency.\n\n## Performance: do we get away with it?\n\nIf we use the built-in AudioContext processing, we are stuck at $w = 32768$ at least. But just how fast are FFTs in today's computers?\nHow much of a hit do we take if we switch to a JS or WASM FFT, compared to the built-in ones? (I'm using a 2020-era \nIntel MacBook Pro as my example). I found an ARM vs Rosetta [benchmark](https://forum.juce.com/t/comparing-fft-engines/46383/2)\nfor Apple's vDSP (which is what Chromium uses internally) that claims a $w=65536$ FFT takes about 1ms on an M1 Mac. Let's say my computer is half as fast as that,\nand that the 32k FFT runs three times as fast; that means I should expect mine to take about 0.6ms. That should actually be fine (!)\nYikes, computers are _fast_ nowadays.\n\nIf we choose to use JS or WASM, [this benchmark](https://toughengineer.github.io/demo/dsp/fft-perf/) gives me\na max window size of 16k, but my laptop appears to be able to process 4200 FFTs of that size per second in pure JS.\nBut if we're doing JS, then we can pre-process our samples (prefiltering and subsampling should work really well\nbecause we know quite precisely the range of frequencies we care about), and we should be totally fine.\nYikes, computers are _fast_ nowadays.\n\n\n\n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"music-analyzer.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"99.9.9","theme":["sandstone","../cscheid-net.scss"],"title":"Notes on musical sound analysis in JS","date":"2022-06-11"},"extensions":{"book":{"multiFile":true}}}}}